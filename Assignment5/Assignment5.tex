\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\usepackage[font=small,skip=3pt]{caption}
\usepackage[margin=1in]{geometry}
\lstset{showstringspaces=false,
		breaklines=true,
		postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookrightarrow\space}}}			


\renewcommand{\thesubsubsection}{\thesubsection.\alph{subsubsection}}

\begin{document}
\title{Intelligent Systems Assignment 5}
\author{Wessel Becker (1982362) \& Sander ten Hoor (2318555)}
\maketitle

\newcommand{\simplesubfigure}[3]{
  \noindent\begin{minipage}{.31\linewidth}
    \begin{center}
      \includegraphics[width=1\linewidth]{#1}
      \captionof{figure}{#2}
    \end{center}
    \label{#3}
  \end{minipage}\hspace{6pt}
}
\newcommand{\simplefigure}[3]{
	\noindent\begin{figure}[H]
  	\centering
    	\makebox[.6\textwidth]
    	{
    		\includegraphics[width=1\textwidth]{#1}
 		} \\
  		\caption{#2}
  		\label{#3}
	\end{figure}
}
\newcommand{\mcode}[1]{
	\lstinputlisting[language=Matlab]{#1}
}

\section{K-means}
\section{Decision tree}
In order to build a decision tree we first need to know what decisions we could make, in order to make this as simple as possible we will only make binary choices. Doing so we decide upon the decision questions in \ref{}
$i \left ( N \right ) = 1 - \max _{j} P\left ( \omega _{j} \right )$\\
$\Delta i \left ( N \right ) = i \left ( N \right ) - P_{L}i\left ( N_{L} \right ) - P_{R}i\left ( N_{R} \right )$\\
Using these formula we can make a decision on what way to order the tree by trying to maximising the impurity drop, so that we always make the decision that reduces the amount of miss classifications the most.\\
$i_{start} = 1 - \frac{1}{5} = \frac{4}{5} $\\
$\Delta i_{fin} = \frac{4}{5} - \left ( \frac{3}{5}  \cdot \frac{2}{3} \right ) - \left ( \frac{2}{5} \cdot \frac{1}{2}\right ) = 0.2$\\
$\Delta i_{fluke} = \frac{4}{5} - \left ( \frac{3}{5}  \cdot \frac{2}{3} \right ) - \left ( \frac{2}{5} \cdot \frac{1}{2}\right ) = 0.2$\\
$\Delta i_{size} = \frac{4}{5} - \left ( \frac{3}{5}  \cdot \frac{2}{3} \right ) - \left ( \frac{2}{5} \cdot \frac{1}{2}\right ) = 0.2$\\
$\Delta i_{tooth} = \frac{4}{5} - \left ( \frac{1}{5}  \cdot \frac{0}{1} \right ) - \left ( \frac{4}{5} \cdot \frac{3}{4}\right ) = 0.2$\\
At this point any question can be chosen, but letâ€™s go with the fluke.
This will give us two possible situations lets start with whales that do not show a fluke. In this case only one question will give us more information and that is whether it has a dorsal fin, if so then it's a killer whale otherwise it's a beluga.
now the other possibility is examined where the whale flukes when it dives. This gives us the following impurity drops.\\
$i_{fluke} = 1 - \frac{1}{3} = \frac{2}{3} $\\
$\Delta i_{fin|fluke} = \frac{2}{3} - \left ( \frac{1}{3}  \cdot \frac{0}{1} \right ) - \left ( \frac{2}{3} \cdot \frac{1}{2}\right ) = \frac{1}{3}$\\
$\Delta i_{size|fluke} = \frac{2}{3} - \left ( \frac{1}{3}  \cdot \frac{0}{1} \right ) - \left ( \frac{2}{3} \cdot \frac{1}{2}\right ) = \frac{1}{3}$\\
$\Delta i_{tooth|fluke} = \frac{2}{3} - \left ( \frac{1}{3}  \cdot \frac{0}{1} \right ) - \left ( \frac{2}{3} \cdot \frac{1}{2}\right ) = \frac{1}{3}$\\
At this leaf anything can be chosen lets go with the presence of a dorsal fin, if this is present we are done other wise there are still two possibilities left namely a Narwhal whale or a Bowhead whale. To make a distinction between those two we can decide either by the presence of a elongated front tooth or by the rather sizeable size difference. Let's choose to go with the size difference. we now end up with the same decision tree as running the c4.5 algorithm in Weka did.

\section{K-nearest neighbours}
\subsection{K-nearest neighbours implementation}
The implementation is availab le in \ref{ap:knn}

\subsection{Classification for different values of K} \label{ss:class}
\ref{ap:knn_img} contains the resulting images. When K = 1, the two groups of points, the green circles and the red crosses, have their own well-defined territory. As values of K increase, more members of a class seem to be in an area which would be classified as the other. This makes sense, as more matches are necessary to come to a satisfactory conclusion. In an area mostly populated by green circles, a new point taking the five closest distances is very likely to end up with more green circles as nearest points than red crosses.

\subsection{Contending Classes}
With the increased number of classes (2 to 4), the effect described in \ref{ss:class} persists, as would be expected.

In case of multiple contending classes, a choice needs to be made. There are four simple options for this:
\begin{itemize}
\item Lowest/Highest class number
\item A random class
\item Class with the closest point
\item Class with the lowest average distance
\end{itemize}

The first option is rather arbitrary and would prioritize specific classes with no other reason than their number. Choosing a random class gets rid of the arbitrary prioritization, but offers no real basis as to why that class was chosen.
Selecting the class with the closest point gives an argument to support the selection of that specific class, but that distance could be a possible outlier, since it is just one. To take everything into account, selecting the class with the lowest average distance gives the most reliable result.

\appendix
\section{figures}
\subsection{K-means}
  \simplesubfigure{./matlab/img/x2}{2 clusters K=2}{fig:x2}%
  \simplesubfigure{./matlab/img/x4}{2 clusters K=4}{fig:x4}%
  \simplesubfigure{./matlab/img/x8}{2 clusters K=8}{fig:x8}%
  \simplesubfigure{./matlab/img/y2}{1 long cluster K=2}{fig:y2}%
  \simplesubfigure{./matlab/img/y4}{1 long cluster K=4}{fig:y4}%
  \simplesubfigure{./matlab/img/y8}{1 long cluster K=8}{fig:y8}%
  \simplesubfigure{./matlab/img/z2}{uniform data K=2}{fig:z12}%
  \simplesubfigure{./matlab/img/z22}{uniform data K=2}{fig:z22}%
  \simplesubfigure{./matlab/img/z4}{uniform data K=4}{fig:z4}%
  \simplesubfigure{./matlab/img/z8}{uniform data K=8}{fig:z18}%
  \simplesubfigure{./matlab/img/z28}{uniform data K=8}{fig:z28}%
\subsection{Decision tree}
\begin{center}
\begin{tabular}{l | l l l l}
\textbf{species} & \textbf{shows fluke when diving} & \textbf{has dorsal fin} & \textbf{is larger then 20 meter} & \textbf{has a elongated tooth}\\
\hline
Killer whale & no & yes & no & no\\
Beluga whale & no & no & no & no\\
Narwal whale & yes & no & no & yes\\
Bowhead whale & yes & no & yes & no\\
Blue whale & yes & yes & yes & no\\
\end{tabular}
\captionof{table}{Possible choices to be made in the classification of whales against the implications of the answers}
\label{tab:1}
\end{center}
\simplefigure{./tree}{decision tree generated with the j48 implementation of the c4.5 algorithm using Weka}{fig:tree}%

\subsection{K-nearest neighbours}\label{ap:knn_img}
\subsubsection{Number of classes = 2}
\simplesubfigure{./matlab/img/3_1_2.png}{K = 1, Nr of classes = 2}{fig:3_1_2}%
\simplesubfigure{./matlab/img/3_3_2.png}{K = 3, Nr of classes = 2}{fig:3_1_2}%
\simplesubfigure{./matlab/img/3_5_2.png}{K = 5, Nr of classes = 2}{fig:3_1_2}%
\simplesubfigure{./matlab/img/3_7_2.png}{K = 7, Nr of classes = 2}{fig:3_1_2}%

\subsubsection{Number of classes = 4}
\simplesubfigure{./matlab/img/3_1_4.png}{K = 1, Nr of classes = 4}{fig:3_1_4}%
\simplesubfigure{./matlab/img/3_3_4.png}{K = 3, Nr of classes = 4}{fig:3_1_4}%
\simplesubfigure{./matlab/img/3_5_4.png}{K = 5, Nr of classes = 4}{fig:3_1_4}%
\simplesubfigure{./matlab/img/3_7_4.png}{K = 7, Nr of classes = 4}{fig:3_1_4}%

\section{code}
\subsection{k-means}
\mcode{./matlab/simpleKMeans.m}
\mcode{./matlab/plotKMeans.m}
\subsection{K-nearest neighbours}\label{ap:knn}
\mcode{./matlab/KNN.m}
\mcode{./matlab/simpleFrequencies.m}

\end{document}