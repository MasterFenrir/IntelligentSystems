\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\usepackage[font=small,skip=3pt]{caption}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks=true]{hyperref}
\lstset{showstringspaces=false,
		breaklines=true,
		postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookrightarrow\space}}}			


\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\begin{document}
\title{Intelligent Systems Assignment 6}
\author{Wessel Becker (1982362) \& Sander ten Hoor (2318555)}
\maketitle

\newcommand{\simplesubfigure}[3]{
  \noindent\begin{minipage}{.31\linewidth}
    \begin{center}
      \includegraphics[width=1\linewidth]{#1}
      \captionof{figure}{#2}
      \label{#3}
    \end{center}
  \end{minipage}\hspace{7pt}
}
\newcommand{\simplefigure}[3]{
	\noindent\begin{figure}[H]
  	\centering
    	\makebox[.6\textwidth]
    	{
    		\includegraphics[height=0.4\textheight]{#1}
 		} \\
  		\caption{#2}
  		\label{#3}
	\end{figure}
}
\newcommand{\mcode}[2]{
	\lstinputlisting[language=Matlab]{#1}\label{#2}
}

\section{Normal Distributions}
\subsection{Scatter plots}
\subsection{Normal distributions}
\subsection{Estimating the prior probabilities}
\subsection{Normalized plots}
\subsection{Solving}\label{ss:solving}
To solve for the decision criteria, we have to solve the following equation:\\
$
P(S1) \frac{1}{{\sigma_1 \sqrt {2\pi } }}e^{{{ - \left( {x - \mu_1 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - \mu_1 } \right)^2 } {2\sigma_1 ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma_1 ^2 }}} = P(S2) \frac{1}{{\sigma_2 \sqrt {2\pi } }}e^{{{ - \left( {x - \mu_2 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - \mu_2 } \right)^2 } {2\sigma_2 ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma_2 ^2 }}}
$\\
For the following values:\\
$P(S1) = 2/3$\\
$\sigma_1 = 12.9024$\\
$\mu_1 = 20.8565$\\
$P(S2) = 1/3$\\
$\sigma_2 = 49.1715$\\
$\mu_2 = 10.3153$\\
Which gives us:\\
$
2/3 * \frac{1}{{12.9024 \sqrt {2\pi } }}e^{{{ - \left( {x - 20.8565 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - 20.85651 } \right)^2 } {2 * 12.9024 ^2 }}} \right. \kern-\nulldelimiterspace} {2 * 12.9024 ^2 }}} = 1/3 * \frac{1}{{49.1715 \sqrt {2\pi } }}e^{{{ - \left( {x - 10.3153 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - 10.3153 } \right)^2 } {2 * 49.1715 ^2 }}} \right. \kern-\nulldelimiterspace} {2 * 49.1715 ^2 }}}
$\\
\\
This is a rather large equation, but Matlab has a function capable of solving mathematical equations. This was used, as seen in \autoref{ss:solver} to obtain the following values as the intersection points of the two normal distributions: $ x = 37.1049$ and $x = 84.7275$.

\subsection{Determining the classes}
Using the obtained decision boundaries, the data points from set \textbf{T} have been classified by giving them a colour. This is visible in \autoref{fig:class}.

\subsection{Misclassification rate}
There are two decision boundaries, as calculated earlier. However, at a decision boundary the normal distribution does not simply stop and there is still a chance that something is misclassified. For example, in \autoref{fig:class} we can see where the two normal distributions clearly cross. At this point, everything to the left of the intersection is classified as \textbf{blue} while everything to the right of it is classified as \textbf{red}. The misclassification rate for the \textbf{red} in this situation is the cumulative probability of the corresponding normal distribution to the left of the intersection. However, this is not all of it. There is a second intersection, as \autoref{ss:solving} showed. To the right of this intersection, everything is classified as \textbf{blue} again. This could lead to potential misclassification again. 

The possible misclassification of a \textbf{blue} point is everything that goes through the \textbf{red} normal distribution. Everything to the left of the first intersection is marked as \textbf{blue} and everything to the right of the second is marked as \textbf{blue}, but everything in between is marked as \textbf{red}. Thus, the total cumulatative probability of this area is the error rate. This is simply calculated by subtracting the cumulatative probability up to the right intersection from the cumulatative probability up to the left intersection. See \autoref{fig:S1_CDF}. The height difference between the two markers is the error rate. This is 0.0817.

The total probability of a misclassification of a \textbf{red} datapoint is the cumulatative probabilities to the left of the first intersection and to the right of the second intersection added together, because everything between those two is already classified as \textbf{red}.
The first one is simple, the cumulatative probability up to 37.1049 using the cumulatative probability function. The second part is the prior probability minus the cumulatative probability up to 84.7275. It is not 1, as the normal distributions are normalized, such that together they have a total probability of 1.
See \autoref{fig:S2_CDF}. The total probability of a missclassification is the prior probability minus value marked in the top right plus the value marked on the bottom. This results in a classification error of 0.0404.

\section{Dendrogram}
For this exercise a dataset of AEX shares was considered. More specifically the data set is de relative change of the stock values per day, for 277 trading days in some year. 

We will try to see if there might be some clusters of similar stocks in this dataset. The first step will be to create a hierarchical clustering of the data and visualize the results. For this visualization we will use a dendogram, which can be seen in figure \ref{fig:dendogram}. For this dendogram a euclidean distance measure was used, however squared euclidean could work as well, when keeping in mind that larger distances would be exaggerated in the resulting dendogram. For the linkage strategy a complete linkage was chosen which will lead to more compact clusters then single linkage, which can lead to very long irregular clusters.

When we take a look at the dendogram we can see that several of the largest companies are boot grouped as very similar to the AEX index as a whole. These are RDS and then AH, HEIN and UNIL, where these last tree are al tree are all closely related to the foods industrie. When 

we start looking for places where we might wan't to make a cut in this dendogram. We could cut of only the AEG stock which seems to be by far the furthest removed from the other stocks. Other cuts that would be interesting would be one at around 0.35 or at around 0.45 euclidean distance between the 277 measurements of the stock growths. Al these cuts would be in places where the higher level clusters would be significantly less compact as the lower clusters. which can be seen by there being allot of vertical space between the branching point in this area. 

\appendix
\section{Figures}
\subsection{Normal Distributions}
\simplefigure{./matlab/img/sc.png}{A scatter plot of the \textbf{S1} one and \textbf{S2} datasets, as well as the new measurements that need to classified.}{fig:scatter}

\simplefigure{./matlab/img/ndb.png}{Here we can see the normal distributions for the \textbf{S1} and \textbf{S2} datasets fitted over the scatter of these datasets}{fig:normal}

\simplefigure{./matlab/img/pp.png}{In this plot we normalized the the probability density functions for the prior probabilities}{fig:normalalizednormal}

\simplefigure{./matlab/img/classified.png}{The datapoints \textbf{T} classified by giving them a colour}{fig:class}

\simplefigure{./matlab/img/S1_CDF.png}{The cumulatative probability for S1, normalized using the prior probability}{fig:S1_CDF}

\simplefigure{./matlab/img/S2_CDF.png}{The cumulatative probability for S2, normalized using the prior probability}{fig:S2_CDF}

\subsection{Hierarchical clustering}
\simplefigure{./images/dendogram.png}{A dendogram of the Hierarchical clustering of the relative changes in stock values of several stock indexes}{fig:dendogram}

\section{Code}
\subsection{Normal Distributions}
\subsubsection{plotScatter.m}
\mcode{./matlab/plotScatter.m}{lst:scatter}
\subsubsection{plotNormalDistributions.m}
\mcode{./matlab/plotNormalDistributions.m}{lst:db}
\subsubsection{priorProbability.m}
\mcode{./matlab/priorProbability.m}{lst:pp}
\subsubsection{plotPosteriorProbability.m}
\mcode{./matlab/plotPosteriorProbability.m}{lst:postp}
\subsubsection{decisionBoundarySolver.m}\label{ss:solver}
\mcode{./matlab/decisionBoundarySolver.m}{lst:dcbs}
\subsubsection{plotClassified.m}
\mcode{./matlab/plotClassified.m}{lst:plclass}
\subsubsection{plotCDF.m}
\mcode{./matlab/plotCDF.m}{lst:plcdf}
\subsubsection{meanAndStdef}
\mcode{./matlab/meanAndStdef.m}{lst:mnstddef}
\subsection{plotDendogram}
\mcode{./matlab/plotDendogram.m}{lst:dendogram}

\subsection{Dendrogram}

\end{document}