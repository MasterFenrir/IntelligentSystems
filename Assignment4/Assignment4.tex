\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\lstset{showstringspaces=false,
		breaklines=true,
		postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookrightarrow\space}}}			


\renewcommand{\thesubsubsection}{\thesubsection.\alph{subsubsection}}

\begin{document}
\title{Intelligent Systems Assignment 4}
\author{Wessel Becker (1982362) \& Sander ten Hoor (2318555)}
\maketitle

\newcommand{\simplefigure}[3]{
	\begin{figure}[H]
  	\centering
    	\makebox[\textwidth]
    	{
    		\includegraphics[width=.6\textwidth]{#1}
 		} \\
  		\caption{#2}
  		\label{#3}
	\end{figure}
}
\newcommand{\mcode}[1]{
	\lstinputlisting[language=Matlab]{#1}
}

\section{1D distrubitions}
In this exercise we examined a dataset of male en female body lengths, the goal being to classify data points into either being of male or of female sex. In order to achieve this in this case we will simply examine the data visually and eyeball a good decision point.

\subsection{Data visualization}
First of we will visualize the data, we do this by drawing a combined graph containing a histogram plot of both heights for women and man. This gives us a clear insight in the way that these heights are distributed as seen figure \ref{fig:1_1}. 

\subsection{Avoiding misclassification on of man}
We then examine the way we could choose to decide whether a new data point is male or female. If we choose the decision criteria as 170 cm with the assumption that everyone larger than 170 cm is male, there would be no men classified incorrectly, for our existing data and if we assume the data to be normally distributed as the histograms seem to indicate, see figure \ref{fig:1_1}. the change of incorrectly assumed males would still stay very low. However, several women would be misclassified. Counting the bars, which suffices in this small sample, representing women right of 170 cm gives us 29 incorrectly classified women, see figure \ref{fig:1_1}, which is allot so unless we find it really important not to miss classify man we can probably do better.

\subsection{minimizing the overall number of miss classifications}
In the case of classification humans by sex we probably want to minimize the overall number of miss classifications on both sides. If the decision criteria would be 178 cm, with everyone larger than 178 cm being male, the lowest number of classification errors would occur according to these histograms. It results in 8 classification errors for women and 4 for men, see figure \ref{fig:1_1}. Making for 12 miss classifications int total much better then the 29 with the previous criteria. It is possible to improve the results for new data points if we consider de way we expect them to be distributed.

\section{2D distributions}
In this exercise a dataset of body heights and hair length's is examined width the goal of finding a classification boundary to classify the dataset into male and female sexes. This is done via visualization.

\subsection{visualization}
In order to get clear visualization of the data which which we are working we make a scatter plot of the data points. In this scatter plot we can clearly see two distinct clusters, see figure \ref{fig:2_1}, which can be assumed to belong to the male and female sex classes.

\subsection{Decision boundary}
In order to determine a decision boundary in this scenario we can also give the fact stated in the exercise that in general men have shorter hair and are taller then women. we also know that we have the same number of male and female data points, neatly coinciding with our two clusters. In order to make as little miss qualifications as possible and with the spread of both cluster being similar. We can then eyeball the line as seen in figure \ref{fig:2_2}. This line was chosen because it divides the two obvious clusters, being as far away from the centres of both clusters as possible. Th line is drawn far away from the cluster centres because this is where the density of data from a certain class is expected to be the highest by keeping the line as far away from these centres we minimize the amount of outlying data points that we miss classify.

\section{Bayes decision rule}
In this exercise we try to determine whether a fish is a sea bass or a salmon. In order to do this we are given the class conditional probability densities for both of these classes of fish. Ass well as the prior probabilities for finding seabass or salmon.

\subsection{Visualisation of posterior probabilities}
In order to get a better insight in the changes of a fish being of a certain class. It is necessary to know the changes of it being of a certain class conditional a given length or in other words we calculate the posterior probabilities. We then plot these in a graph so that we can estimate the changes of a new fish being of a certain class by eye balling this graph, see figure \ref{fig:3_1}.

\subsection{Making decisions}
Since we now have a way to see the changes of a new fish of a certain size belonging to either of these two classes. One thing is still needed, this is a rule by which we decide in which of the two classes a fish belongs. For this we will uses Bayes decision rule which comes down to us always choosing the class which has the highest posterior probability. Using this rule both fishes, of 8 cm and 20 cm, would be classified as a seabass, since for these given lengths seabass has the highest posterior probability. However, the expected error is higher for the fish of 8 cm, since the posterior probabilities are closer to each other than at 20 cm. It is also evident now when we look at figure \ref{fig:3_1} that there will be a single decision boundary somewhere around 22 cm after which it becomes more likely that a new found specimen of that fish is a salmon.

\section{Iris scan}
In this exercise a set of normalized iris scan data is given. The goal being to decide on a classification rule for an authorization system, deciding wether it is save to assume two data points belong to the same person.

\subsection{First look}
Taken a quick look at the data we can see that when we compare two data points from different persons roughly half the binary values between the two scans are different. Which is what we expect for two randomly generated binary vectors. When we compare two vectors belonging to the same person we notice that the number of different values is allot lower, with only a few values being different. This means that it will be possible to make a highly reliable authentication system with this data without having tom many false negatives.

\subsection{visualization distance}
In order to to get an idea of the distribution of the difference between two scans of the same person and two scans of a different person. This is achieved by generating histograms for 1000 randomly chosen stets of scans belonging to the same person and 1000 randomly chosen sets of scans belonging to different persons.The code for selection these sets can be found in Appendix \ref{a:plotHDHistogram}. 

\subsection{Visualization histograms}
In order to visualize the distribution of theses distances a graph can be made containing two histograms corresponding to the different and same person distance sets. The histograms can be found in figure \ref{fig:4_1}.

\subsection{Visualization probability density functions}
When we look at histograms we created in figure \ref{fig:4_1} we notice that this data is looks pretty clearly normally distributed. Using this assumption we can now also plot the normal distributions corresponding to the two sets of distances. Doing so gives the graph in figure \ref{fig:4_2}. 

\subsection{Making a decision}
The final step we need to take is to decide on some decision criteria based on which some set of input data is accepted as authentic. In this case there is no interest in the posterior probability, since we assume the system to be attacked by a malafide entity anyway, making the prior probability of a different person versus a same person using the system irrelevant. It is however the goal to ensure by some level of certainty that a malfide attempt to authenticate fails. 

In other words if our null hypothesis is that two measurements belong to different persons and our alternative hypothesis is that two measurements belong to the same person than we want to minimize the type 2 error, the incorrect rejection of the null hypothesis. 

We choose therefore a very small alpha of 0.0005, we can then compute the distance value for which this value of alpha is achieved. the alpha value being given by the cumulative distribution for different person. We can find value x for which p(x) is alpha by computing F'(x) using the erfinv function in matlab. This gives us a x value of 0.2089. If a value we find is lower or equal to this value we can assume the alternative hypothesis to be true with a very reasonable amount of doubt.

We can now also compute the type 2 error by inputting the resulting value x into the cdf of the same persons dataset, giving us 0.00085 for the type 2 error rate. A more visual representation of the two computations we did to obtain x and the type 2 error can be seen in figure \ref{fig:4_3}

\section{Work done}
The programming for exercise 1, 2 and 3 were done by Sander. Wessel did the programming for exercise 4. The text was written as a combined effort, with Wessel having written more than Sander.

\appendix
\section{graphs}
\subsection{Exercise 1 graphs}
\simplefigure{./images/1_1}{Histogram of height, created using \ref{a:plotHistogram}}{fig:1_1}

\subsection{Exercise 2 graphs}
\simplefigure{./images/2_1}{Body length vs hair length, created using \ref{a:plotLengthAgainstHair}}{fig:2_1}
\simplefigure{./images/2_2}{The decision boundary}{fig:2_2}

\subsection{Exercise 3 graphs}
\simplefigure{./images/3_1}{Posterior probablity, created using \ref{a:plotFish}}{fig:3_1}

\subsection{Exercise 4 graphs}
\simplefigure{./images/4_3_hist_only}{Histograms of the iris HD created using \ref{a:plotHDHistogram}}{fig:4_1}

\simplefigure{./images/4_3}{Histograms of the iris HD, combined with their matching Normal Distributions}{fig:4_2}

\simplefigure{./images/4_4}
{Cumulative Distribution Function displaying the cumulative chance of misclassification. Made using \ref{a:plotCDF}}{fig:4_3}

\section{Matlab code}
\subsection{plotHistogram.m}\label{a:plotHistogram}
\mcode{./matlab/plotHistogram.m}

\subsection{plotLengthAgainstHair.m}\label{a:plotLengthAgainstHair}
\mcode{./matlab/plotLengthAgainstHair.m}

\subsection{plotFish.m}\label{a:plotFish}
\mcode{./matlab/plotFish.m}

\subsection{computeNormalDistribution.m}\label{a:computeND}
\mcode{./matlab/computeNormalDistribution.m}

\subsection{computeHammingDistance.m}\label{a:computeHD}
\mcode{./matlab/computeHammingDistance.m}

\subsection{computeDecisionBoundary.m}\label{a:computeDB}
\mcode{./matlab/computeDecisionBoundary.m}

\subsection{plotHDHistogram.m}\label{a:plotHDHistogram}
\mcode{./matlab/plotHDHistogram.m}

\subsection{plotDifferentPersonsCDF.m}\label{a:plotCDF}
\mcode{./matlab/plotDifferentPersonsCDF.m}

\end{document}